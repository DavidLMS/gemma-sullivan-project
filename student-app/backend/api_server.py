"""
FastAPI Server for Student App Backend
Serves educational content generated by Gemma AI system
"""

import asyncio
import base64
import json
import logging
import os
import shutil
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from contextlib import asynccontextmanager
from concurrent.futures import ProcessPoolExecutor

import uvicorn
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileCreatedEvent, FileModifiedEvent
from model_service import create_model_service
from parsers import parse_answer_evaluation
from parsers import parse_educational_textbook, parse_educational_story
from dotenv import load_dotenv
from xapi_logger import xapi_logger
from feedback_queue import (
    initialize_feedback_queue, 
    stop_feedback_queue, 
    create_feedback_task, 
    get_task_status,
    get_queue_stats,
    cleanup_old_tasks,
    TASK_STATUS_PENDING,
    TASK_STATUS_PROCESSING, 
    TASK_STATUS_COMPLETED,
    TASK_STATUS_ERROR
)
from discovery_service import get_discovery_service
from automatic_questions_service import automatic_questions_service
from sync_client import sync_client
from student_profile import (
    get_current_student_profile, 
    get_student_profile_for_content_generation
)

# Load environment variables (override=True to overwrite any already loaded values)
load_dotenv(override=True, verbose=True)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Verify sync_client import
try:
    if hasattr(sync_client, 'tutor_url'):
        logger.info(f"✅ sync_client imported successfully. Tutor URL: {sync_client.tutor_url}")
    else:
        logger.error("❌ sync_client imported but missing tutor_url attribute")
except NameError:
    logger.error("❌ sync_client not imported - sync endpoints will not work")
except Exception as e:
    logger.error(f"❌ Error with sync_client: {e}")

# DEBUG: Log the loaded values to verify they're working
# USE_MLX_VLM environment variable loaded

# Directory paths
CONTENT_DIR = Path("content")
INBOX_DIR = CONTENT_DIR / "inbox"
PROCESSED_DIR = CONTENT_DIR / "processed"
GENERATED_DIR = CONTENT_DIR / "generated"
TEXTBOOKS_DIR = GENERATED_DIR / "learn" / "textbooks"
STORIES_DIR = GENERATED_DIR / "learn" / "stories"
EXPERIMENT_UPLOADS_DIR = CONTENT_DIR / "experiment_uploads"
PROGRESS_FILE = CONTENT_DIR / "progress.json"
PROFILE_FILE = Path(__file__).parent / "profile.json"

# DEBUG: Log startup directories and paths
# Server starting from directory logged
# API server file location logged
# Profile save location logged

# SSE Notification System
class SSENotificationManager:
    def __init__(self):
        self.clients = []
        
    def add_client(self, client_queue: asyncio.Queue):
        """Add a client to receive notifications"""
        self.clients.append(client_queue)
        logger.info(f"SSE client connected. Total clients: {len(self.clients)}")
        
    def remove_client(self, client_queue: asyncio.Queue):
        """Remove a client"""
        if client_queue in self.clients:
            self.clients.remove(client_queue)
            logger.info(f"SSE client disconnected. Total clients: {len(self.clients)}")
    
    async def notify_content_change(self, event_type: str, content_id: str = None, data: dict = None):
        """Send notification to all connected clients"""
        event_data = {
            "type": event_type,
            "timestamp": datetime.now().isoformat(),
            "content_id": content_id,
            "data": data or {}
        }
        
        logger.info(f"SSE notification: {event_type} for {content_id}")
        
        # Send to all connected clients
        disconnected_clients = []
        for client_queue in self.clients:
            try:
                await client_queue.put(event_data)
            except Exception as e:
                # Client queue error, mark for removal
                logger.warning(f"Failed to send to client: {e}")
                disconnected_clients.append(client_queue)
        
        # Clean up disconnected clients
        for client in disconnected_clients:
            self.remove_client(client)
    
    async def notify_sync_change(self, is_available: bool):
        """Send sync status change notification to all connected clients"""
        event_type = "tutor_sync_enabled" if is_available else "tutor_sync_disabled"
        event_data = {
            "type": event_type,
            "timestamp": datetime.now().isoformat(),
            "sync_available": is_available
        }
        
        logger.info(f"SSE sync notification: {event_type}")
        
        # Send to all connected clients
        disconnected_clients = []
        for client_queue in self.clients:
            try:
                await client_queue.put(event_data)
            except Exception as e:
                # Client queue error, mark for removal
                logger.warning(f"Failed to send sync notification to client: {e}")
                disconnected_clients.append(client_queue)
        
        # Clean up disconnected clients
        for client in disconnected_clients:
            self.remove_client(client)

# Global SSE manager
sse_manager = SSENotificationManager()

# Global sync status tracking
previous_sync_status = None

# Global file watcher variables
file_watcher_observer = None
model_service = None
model_service_lock = asyncio.Lock()
main_event_loop = None

# Global processing queue and worker
processing_queue = None
processing_worker_task = None
# Global set to track files being processed
processing_files = set()
# Global process executor for Gemma processing
process_executor = None

async def notify_sse(event_type: str, content_id: str = None, data: dict = None):
    """Send SSE notification using the global sse_manager"""
    try:
        logger.info(f"Sending SSE notification: {event_type} for {content_id} to {len(sse_manager.clients)} clients")
        await sse_manager.notify_content_change(event_type, content_id, data)
        logger.info(f"SSE notification sent successfully: {event_type} for {content_id}")
    except Exception as e:
        logger.error(f"Failed to send SSE notification {event_type} for {content_id}: {e}")
        import traceback
        logger.error(f"SSE error traceback: {traceback.format_exc()}")

def notify_sse_sync(event_type: str, content_id: str = None, data: dict = None):
    """Synchronous wrapper for notify_sse - schedules async call from thread"""
    global main_event_loop
    
    try:
        if main_event_loop and main_event_loop.is_running():
            logger.info(f"Scheduling SSE notification: {event_type} for {content_id}")
            # Schedule the coroutine in the main event loop (thread-safe)
            future = asyncio.run_coroutine_threadsafe(
                notify_sse(event_type, content_id, data), main_event_loop
            )
            
            # Wait briefly for the notification to be scheduled (non-blocking)
            try:
                # Wait up to 0.1 seconds for the notification to be queued
                result = future.result(timeout=0.1)
                logger.info(f"SSE notification scheduled successfully: {event_type} for {content_id}")
            except Exception as e:
                # Don't fail if we can't wait for result, just log
                logger.warning(f"SSE notification may be delayed: {event_type} for {content_id} - {e}")
        else:
            logger.error(f"Main event loop not available for SSE notification: {event_type} for {content_id}")
            logger.error(f"Event loop state - exists: {main_event_loop is not None}, running: {main_event_loop.is_running() if main_event_loop else False}")
                
    except Exception as e:
        logger.error(f"Failed to send SSE notification {event_type} for {content_id}: {e}")
        import traceback
        logger.error(f"SSE notification error traceback: {traceback.format_exc()}")

async def get_model_service():
    """Get model service instance with lazy loading"""
    global model_service
    
    # Use async lock to ensure only one initialization happens at a time
    async with model_service_lock:
        if model_service is None:
            logger.info("Initializing Gemma model service...")
            
            def init_and_load_model():
                service = create_model_service()
                service.load_model()  # Important: Load the model after initialization
                return service
            
            # Run the model initialization and loading in a thread to avoid blocking
            loop = asyncio.get_event_loop()
            try:
                model_service = await loop.run_in_executor(None, init_and_load_model)
                logger.info("Gemma model service initialized and loaded successfully")
            except Exception as e:
                logger.error(f"Failed to initialize and load Gemma model service: {e}")
                raise e
                
    return model_service

def get_student_profile_from_env():
    """Get student profile from centralized system - DEPRECATED, use get_current_student_profile() directly"""
    return get_current_student_profile()

def get_model_service_sync():
    """Synchronous wrapper to get model service from thread"""
    try:
        # Create new event loop for this thread if needed
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            # No event loop in current thread, create one
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        # Run the async function
        return loop.run_until_complete(get_model_service())
    except Exception as e:
        logger.error(f"Failed to get model service: {e}")
        raise e

class ContentFileHandler(FileSystemEventHandler):
    """File system event handler for content inbox monitoring"""
    
    def __init__(self, processing_queue):
        super().__init__()
        self.inbox_dir = Path(os.getenv("INBOX_DIR", "content/inbox"))
        self.processed_dir = Path(os.getenv("PROCESSED_DIR", "content/processed"))
        self.logs_dir = Path(os.getenv("LOGS_DIR", "logs"))
        self.textbooks_dir = Path("content/generated/learn/textbooks")
        self.stories_dir = Path("content/generated/learn/stories")
        self.processing_queue = processing_queue  # Queue for background processing
        
        # Ensure directories exist
        self.inbox_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        self.logs_dir.mkdir(parents=True, exist_ok=True)
        self.textbooks_dir.mkdir(parents=True, exist_ok=True)
        self.stories_dir.mkdir(parents=True, exist_ok=True)
        
    def on_created(self, event):
        """Handle file creation events"""
        if isinstance(event, FileCreatedEvent) and not event.is_directory:
            if event.src_path.endswith('.txt'):
                logger.info(f"New file detected: {event.src_path}")
                
                # Extract content name for SSE notification
                content_id = Path(event.src_path).stem
                logger.info(f"About to send file_detected SSE for: {content_id}")
                notify_sse_sync("file_detected", content_id, {"status": "queued"})
                logger.info(f"file_detected SSE call completed for: {content_id}")
                
                self._queue_file_for_processing(event.src_path)
    
    def on_modified(self, event):
        """Handle file modification events"""
        if isinstance(event, FileModifiedEvent) and not event.is_directory:
            if event.src_path.endswith('.txt'):
                # Small delay to ensure file is fully written
                time.sleep(1)
                logger.info(f"File modified: {event.src_path}")
                self._queue_file_for_processing(event.src_path)
    
    def _queue_file_for_processing(self, file_path):
        """Queue a file for background processing (non-blocking)"""
        global processing_files
        file_path = Path(file_path)
        
        # Avoid queueing the same file multiple times
        if str(file_path) in processing_files:
            logger.info(f"File {file_path} already queued/processing, skipping")
            return
        
        # Basic validation before queueing
        if not file_path.exists():
            logger.warning(f"File {file_path} no longer exists")
            return
        
        # Check if file is in inbox
        if not str(file_path.resolve()).startswith(str(self.inbox_dir.resolve())):
            logger.info(f"File {file_path} not in inbox, skipping")
            return
        
        # Mark as processing and queue it
        processing_files.add(str(file_path))
        
        # Queue for background processing
        if self.processing_queue:
            try:
                self.processing_queue.put_nowait(str(file_path))
                logger.info(f"File {file_path} queued for processing")
            except Exception as e:
                logger.error(f"Failed to queue file {file_path}: {e}")
                processing_files.discard(str(file_path))
        else:
            logger.warning(f"No processing queue available for {file_path}")
    
    def _process_file(self, file_path):
        """Process a .txt file with Gemma model"""
        global processing_files
        file_path = Path(file_path)
        
        # Note: File is already marked as processing in _queue_file_for_processing()
        # No need to check or add again here
        
        try:
            
            # Check if file exists and is in inbox
            if not file_path.exists():
                logger.warning(f"File {file_path} no longer exists")
                return
            
            # Use resolved paths for comparison
            if not str(file_path.resolve()).startswith(str(self.inbox_dir.resolve())):
                logger.info(f"File {file_path} not in inbox, skipping")
                return
            
            # Extract content ID and notify processing started
            content_id = file_path.stem
            notify_sse_sync("processing_started", content_id, {"status": "building"})
            
            # Read file content
            logger.info(f"Reading content from: {file_path}")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
            except Exception as e:
                logger.error(f"Error reading file {file_path}: {e}")
                return
            
            if not content:
                logger.warning(f"File {file_path} is empty, skipping")
                return
            
            logger.info(f"File content length: {len(content)} characters")
            
            # Get student profile from centralized system
            student_profile = get_student_profile_for_content_generation(content)
            
            logger.info(f"Processing with Gemma for student: {student_profile['student_name']}")
            logger.info(f"Student profile: {student_profile['student_age']} years, {student_profile['student_course']}, interested in {student_profile['student_interests']}")
            
            # Generate textbook format
            logger.info("=" * 50)
            logger.info("GENERATING TEXTBOOK FORMAT")
            logger.info("=" * 50)
            textbook_result = self._generate_format(
                file_path, "educational_textbook", parse_educational_textbook, 
                student_profile, "textbook"
            )
            
            # Generate story format
            logger.info("=" * 50)
            logger.info("GENERATING STORY FORMAT")
            logger.info("=" * 50)
            story_result = self._generate_format(
                file_path, "educational_storytelling", parse_educational_story, 
                student_profile, "story"
            )
            
            # Move file to processed directory
            processed_path = self.processed_dir / file_path.name
            
            # Handle name conflicts
            counter = 1
            while processed_path.exists():
                stem = file_path.stem
                suffix = file_path.suffix
                processed_path = self.processed_dir / f"{stem}_{counter}{suffix}"
                counter += 1
            
            try:
                shutil.move(str(file_path), str(processed_path))
                logger.info(f"✓ Moved to processed: {processed_path}")
                
                # Notify processing completed successfully
                notify_sse_sync("processing_completed", content_id, {
                    "status": "available",
                    "textbook_success": textbook_result is not None,
                    "story_success": story_result is not None
                })
                
            except Exception as e:
                logger.error(f"Error moving file to processed: {e}")
                # Notify processing failed
                notify_sse_sync("processing_failed", content_id, {"status": "error", "error": str(e)})
                
        except Exception as e:
            logger.error(f"Unexpected error processing file {file_path}: {e}")
            # Notify processing failed
            content_id = file_path.stem if 'content_id' not in locals() else content_id
            notify_sse_sync("processing_failed", content_id, {"status": "error", "error": str(e)})
            
        finally:
            processing_files.discard(str(file_path))
    
    def _generate_format(self, file_path, prompt_name, parser_func, student_profile, format_type):
        """Generate content in specific format (textbook or story)"""
        try:
            # Get model service with lazy loading
            model_service = get_model_service_sync()
            
            # Load prompt template
            prompt_template = model_service.load_prompt(prompt_name)
            
            # Get retry configuration from environment (same as feedback generation)
            max_retries = int(os.getenv("MAX_RETRIES", "3"))
            logger.info(f"{format_type.title()} generation will attempt up to {max_retries} retries for parsing")
            
            # Generate content with parser and explicit retry parameters
            result = model_service.generate(
                prompt_template=prompt_template,
                variables=student_profile,
                parser_func=parser_func,
                max_retries=max_retries
            )
            
            if isinstance(result, dict):
                logger.info(f"✓ Successfully generated {format_type}: {result['total_sections']} sections")
                
                # Save individual sections to appropriate directory
                output_dir = self.textbooks_dir if format_type == "textbook" else self.stories_dir
                self._save_sections(file_path, result, output_dir, format_type)
                
                return result
            else:
                logger.warning(f"✗ {format_type.capitalize()} parsing failed, got raw text")
                logger.info(f"Raw {format_type} preview: {str(result)[:200]}...")
                return {"error": "parsing_failed", "raw_content": result}
                
        except Exception as e:
            logger.error(f"Error generating {format_type}: {e}")
            return {"error": str(e)}
    
    def _save_sections(self, file_path, parsed_result, output_dir, format_type):
        """Save parsed sections to individual files"""
        base_name = file_path.stem
        
        # Save as JSON with same name as original file
        json_file = output_dir / f"{base_name}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(parsed_result, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved {format_type} to: {json_file}")
    

async def file_processing_worker():
    """Background worker that processes files from the queue sequentially"""
    global processing_queue
    
    logger.info("File processing worker started")
    
    while True:
        try:
            # Wait for a file to process
            file_path_str = await processing_queue.get()
            file_path = Path(file_path_str)
            content_id = file_path.stem
            
            logger.info(f"Processing worker: Starting processing of {file_path}")
            notify_sse_sync("processing_started", content_id, {"status": "building"})
            
            # Run the file processing in a separate process to avoid blocking the event loop
            # This prevents Gemma from blocking the main event loop
            loop = asyncio.get_event_loop()
            success = await loop.run_in_executor(process_executor, _process_file_sync, file_path_str)
            
            if success:
                logger.info(f"Processing worker: Completed processing of {file_path}")
                notify_sse_sync("processing_completed", content_id, {
                    "status": "available",
                    "textbook_success": True,
                    "story_success": True
                })
            else:
                logger.error(f"Processing worker: Failed processing of {file_path}")
                notify_sse_sync("processing_failed", content_id, {
                    "status": "error", 
                    "error": "Processing failed"
                })
            
            # Mark task as done
            processing_queue.task_done()
            
        except Exception as e:
            logger.error(f"File processing worker error: {e}")
            # Make sure to clean up processing set even on error
            if 'file_path_str' in locals():
                processing_files.discard(file_path_str)
            # Continue processing other files even if one fails
            continue

def _process_file_sync(file_path_str):
    """Synchronous wrapper for file processing - runs in thread executor"""
    try:
        # Create a temporary handler instance to access the _process_file method
        handler = ContentFileHandler(None)  # Queue not needed for sync processing
        handler._process_file(file_path_str)
        return True
    except Exception as e:
        logger.error(f"Sync file processing failed for {file_path_str}: {e}")
        return False

async def start_file_watcher():
    """Initialize and start the file watcher as a background task"""
    global file_watcher_observer, processing_queue, processing_worker_task, process_executor
    
    try:
        logger.info("Starting file watcher...")
        
        # Initialize processing queue
        processing_queue = asyncio.Queue(maxsize=50)  # Allow up to 50 files in queue
        
        # Initialize process executor for Gemma processing (prevents event loop blocking)
        process_executor = ProcessPoolExecutor(max_workers=1)
        logger.info("ProcessPoolExecutor initialized for Gemma processing")
        
        # Start background processing worker
        processing_worker_task = asyncio.create_task(file_processing_worker())
        
        # Create file handler with queue reference
        handler = ContentFileHandler(processing_queue)
        file_watcher_observer = Observer()
        file_watcher_observer.schedule(handler, str(INBOX_DIR), recursive=False)
        file_watcher_observer.start()
        logger.info(f"File watcher started, monitoring: {INBOX_DIR.resolve()}")
        logger.info("Background file processing worker started")
        
    except Exception as e:
        logger.error(f"Failed to start file watcher: {e}")

async def stop_file_watcher():
    """Stop the file watcher"""
    global file_watcher_observer, processing_worker_task, processing_queue, process_executor
    
    if file_watcher_observer:
        logger.info("Stopping file watcher...")
        file_watcher_observer.stop()
        file_watcher_observer.join()
        logger.info("File watcher stopped")
    
    # Stop the background processing worker
    if processing_worker_task:
        logger.info("Stopping background processing worker...")
        processing_worker_task.cancel()
        try:
            await processing_worker_task
        except asyncio.CancelledError:
            logger.info("Background processing worker stopped")
    
    # Shutdown the process executor
    if process_executor:
        logger.info("Shutting down ProcessPoolExecutor...")
        process_executor.shutdown(wait=True)
        logger.info("ProcessPoolExecutor stopped")
    
    # Clear the processing queue
    if processing_queue:
        processing_queue = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan context manager for startup/shutdown"""
    global main_event_loop
    
    # Startup
    logger.info("Starting application...")
    # Store reference to main event loop for thread communication
    main_event_loop = asyncio.get_event_loop()
    
    # Initialize feedback queue system
    await initialize_feedback_queue()
    logger.info("Feedback queue system initialized")
    
    await start_file_watcher()
    
    yield  # App runs here
    
    # Shutdown
    logger.info("Shutting down application...")
    await stop_file_watcher()
    await stop_feedback_queue()
    logger.info("Feedback queue system stopped")
    main_event_loop = None

# Create FastAPI app with lifespan
app = FastAPI(title="Student App API", version="1.0.0", lifespan=lifespan)

# Add CORS middleware for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173"],  # React dev servers
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class ContentItem(BaseModel):
    id: str
    name: str
    displayName: str
    status: str  # 'building' | 'available'
    totalSections: Optional[int] = None
    viewedSections: Optional[int] = None
    lastViewed: Optional[str] = None

class ProgressUpdate(BaseModel):
    viewedSections: int
    format: str
    currentSection: int

class PracticeAnswer(BaseModel):
    questionId: str
    isCorrect: bool
    questionType: str
    contentId: str
    userAnswer: Optional[str] = None  # Student's answer for AI evaluation

class LearnContent(BaseModel):
    type: str
    total_sections: int
    sections: List[Dict]

class ExperimentDecision(BaseModel):
    challengeId: str
    decision: str  # "accepted" or "rejected"

class ExperimentSession(BaseModel):
    challengeId: str
    textContent: str
    activeTab: str  # "text" | "draw" | "upload"
    uploadedFiles: Optional[List[str]] = []
    currentCanvas: Optional[str] = None  # Single Base64 encoded canvas state


class ChallengeSubmission(BaseModel):
    challengeId: str
    textContent: str
    canvasData: Optional[str] = None  # Base64 encoded PNG
    continueRefining: Optional[bool] = None  # Only present in feedback response

class ChallengeFeedbackDecision(BaseModel):
    challengeId: str
    taskId: str
    continueRefining: bool


class DiscoveryAnalyzeRequest(BaseModel):
    imageData: str  # Base64 encoded image
    questionText: Optional[str] = None  # Text question (if no audio)
    audioData: Optional[str] = None  # Base64 encoded audio (optional)


class DiscoveryQuestionRequest(BaseModel):
    sessionId: str  # Using sessionId field for investigation_id (compatibility)
    responseText: str  # Using responseText field for selected question


class DiscoveryRevealRequest(BaseModel):
    investigationId: str


class DiscoveryCompleteRequest(BaseModel):
    investigationId: str
    selectedAnswer: str  # Student's final answer choice

# Student Profile Management Models
class StudentProfile(BaseModel):
    id: str              # 6-digit student ID
    name: str            # Full name
    age: int             # Student age
    grade: str           # Grade/course level
    language: str        # Preferred language code (en, es, fr, etc.)
    interests: Optional[str] = ""  # Student interests (comma-separated)
    created_at: str      # Profile creation timestamp
    completed_onboarding: bool = True

class ProfileUpdateRequest(BaseModel):
    profile: StudentProfile

class StudentIdValidationRequest(BaseModel):
    id: str

class StudentIdValidationResponse(BaseModel):
    available: bool
    error: Optional[str] = None

class ProfileResponse(BaseModel):
    profile: Optional[StudentProfile]
    success: bool
    error: Optional[str] = None


def save_profile_json(profile: StudentProfile):
    """Save student profile to profile.json file"""
    profile_file = PROFILE_FILE
    
    # DEBUG: Log detailed path information
    # Current working directory during save
    # Profile file variable logged
    # Profile file absolute path logged
    # Profile file exists check
    
    try:
        # Convert profile to dictionary
        profile_data = {
            "id": profile.id,
            "name": profile.name,
            "age": profile.age,
            "grade": profile.grade,
            "language": profile.language,
            "interests": profile.interests or "",
            "created_at": profile.created_at,
            "completed_onboarding": profile.completed_onboarding
        }
        
        # Write to JSON file
        with open(profile_file, 'w', encoding='utf-8') as f:
            json.dump(profile_data, f, indent=2, ensure_ascii=False)
            
        # DEBUG: Verify file was written where expected
        logger.info(f"Successfully saved student profile to {profile_file.absolute()} for student {profile.id}")
        # File exists after write check
        # File size after write logged
        
    except Exception as e:
        logger.error(f"Error saving profile.json: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save profile: {str(e)}")

def get_current_profile() -> Optional[StudentProfile]:
    """Get current student profile from profile.json file"""
    profile_file = PROFILE_FILE
    
    try:
        # Check if profile file exists
        if not profile_file.exists():
            return None
            
        # Read and parse JSON file
        with open(profile_file, 'r', encoding='utf-8') as f:
            profile_data = json.load(f)
        
        # Convert to StudentProfile object
        return StudentProfile(
            id=profile_data["id"],
            name=profile_data["name"],
            age=profile_data["age"],
            grade=profile_data["grade"],
            language=profile_data["language"],
            interests=profile_data.get("interests", ""),
            created_at=profile_data.get("created_at", datetime.now().isoformat()),
            completed_onboarding=profile_data.get("completed_onboarding", True)
        )
    except Exception as e:
        logger.error(f"Error reading profile.json: {e}")
        return None

async def validate_student_id_with_tutor(student_id: str) -> bool:
    """Validate if student ID is available in tutor-app"""
    try:
        # Try to connect to tutor service
        tutor_url = os.getenv('TUTOR_SERVICE_URL', 'http://localhost:8001')
        
        import requests
        response = requests.get(f"{tutor_url}/api/students", timeout=5)
        
        if response.ok:
            students_data = response.json()
            existing_ids = [student.get('id') for student in students_data.get('students', [])]
            return student_id not in existing_ids
        else:
            # If tutor service is not available, assume ID is available
            logger.warning("Tutor service not available for ID validation")
            return True
            
    except Exception as e:
        logger.warning(f"Could not validate ID with tutor service: {e}")
        # If validation fails, assume ID is available
        return True

async def register_student_with_tutor(profile: StudentProfile) -> bool:
    """Register student with tutor-app"""
    try:
        tutor_url = os.getenv('TUTOR_SERVICE_URL', 'http://localhost:8001')
        
        import requests
        student_data = {
            'id': profile.id,
            'name': profile.name
        }
        
        response = requests.post(
            f"{tutor_url}/api/students",
            json=student_data,
            timeout=10
        )
        
        if response.ok:
            logger.info(f"Successfully registered student {profile.id} with tutor-app")
            return True
        else:
            logger.warning(f"Failed to register student with tutor-app: {response.status_code}")
            return False
            
    except Exception as e:
        logger.warning(f"Could not register student with tutor service: {e}")
        return False


def format_display_name(name: str) -> str:
    """Format filename to display name"""
    return name.replace('_', ' ').title()

def load_progress_data() -> Dict:
    """Load progress data from JSON file"""
    if not PROGRESS_FILE.exists():
        return {}
    
    try:
        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading progress data: {e}")
        return {}

def save_progress_data(data: Dict):
    """Save progress data to JSON file"""
    try:
        CONTENT_DIR.mkdir(parents=True, exist_ok=True)
        with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Error saving progress data: {e}")

# Practice progress management
PRACTICE_PROGRESS_FILE = Path("content/practice_progress.json")

def load_practice_progress():
    """Load practice progress data from JSON file"""
    try:
        if PRACTICE_PROGRESS_FILE.exists():
            with open(PRACTICE_PROGRESS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {}
    except Exception as e:
        logger.error(f"Error loading practice progress: {e}")
        return {}

def save_practice_progress(progress_data):
    """Save practice progress data to JSON file"""
    try:
        CONTENT_DIR.mkdir(parents=True, exist_ok=True)
        with open(PRACTICE_PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, indent=2, ensure_ascii=False)
        logger.info(f"Practice progress saved to {PRACTICE_PROGRESS_FILE}")
    except Exception as e:
        logger.error(f"Error saving practice progress: {e}")

# Experiment progress management
EXPERIMENT_PROGRESS_FILE = Path("content/experiment_progress.json")

def load_experiment_progress():
    """Load experiment progress data from JSON file"""
    try:
        if EXPERIMENT_PROGRESS_FILE.exists():
            with open(EXPERIMENT_PROGRESS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {'accepted': [], 'rejected': [], 'last_session': None}
    except Exception as e:
        logger.error(f"Error loading experiment progress: {e}")
        return {'accepted': [], 'rejected': [], 'last_session': None}

def save_experiment_progress(progress_data):
    """Save experiment progress data to JSON file"""
    try:
        CONTENT_DIR.mkdir(parents=True, exist_ok=True)
        with open(EXPERIMENT_PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, indent=2, ensure_ascii=False)
        logger.info(f"Experiment progress saved to {EXPERIMENT_PROGRESS_FILE}")
    except Exception as e:
        logger.error(f"Error saving experiment progress: {e}")

def get_inbox_files() -> List[str]:
    """Get files currently being processed (in inbox)"""
    if not INBOX_DIR.exists():
        return []
    
    return [f.name for f in INBOX_DIR.glob("*.txt")]

def get_processed_files() -> List[str]:
    """Get files that have been processed"""
    if not PROCESSED_DIR.exists():
        return []
    
    return [f.name for f in PROCESSED_DIR.glob("*.txt")]

def get_available_content() -> List[str]:
    """Get content that has generated textbooks available"""
    if not PROCESSED_DIR.exists():
        return []
    
    # Get base content names from processed files
    processed_files = [f.stem for f in PROCESSED_DIR.glob("*.txt")]
    available_content = []
    
    # Check if each processed file has at least one generated format
    # Support both naming patterns: {content_name}_{format}_timestamp.json and {content_name}.json
    for content_name in processed_files:
        has_textbook = False
        has_story = False
        
        if TEXTBOOKS_DIR.exists():
            # Check for timestamped pattern first, then simple pattern
            has_textbook = (any(f.stem.startswith(f"{content_name}_textbook") for f in TEXTBOOKS_DIR.glob("*.json")) or
                           any(f.stem == content_name for f in TEXTBOOKS_DIR.glob("*.json")))
            
        if STORIES_DIR.exists():
            # Check for timestamped pattern first, then simple pattern
            has_story = (any(f.stem.startswith(f"{content_name}_story") for f in STORIES_DIR.glob("*.json")) or
                        any(f.stem == content_name for f in STORIES_DIR.glob("*.json")))
        
        if has_textbook or has_story:
            available_content.append(content_name)
    
    return available_content

def find_content_file(content_id: str, format_type: str) -> Optional[Path]:
    """Find the actual file for a content ID and format"""
    if format_type == "textbook":
        search_dir = TEXTBOOKS_DIR
        prefix = f"{content_id}_textbook"
    elif format_type == "story":
        search_dir = STORIES_DIR  
        prefix = f"{content_id}_story"
    else:
        return None
    
    if not search_dir.exists():
        return None
    
    # Find file that starts with the prefix (timestamped pattern)
    for file_path in search_dir.glob("*.json"):
        if file_path.stem.startswith(prefix):
            return file_path
    
    # If not found, try simple pattern (content_id.json)
    simple_file = search_dir / f"{content_id}.json"
    if simple_file.exists():
        return simple_file
    
    return None

def get_available_formats(content_id: str) -> Dict[str, bool]:
    """Get available formats for a content item"""
    has_textbook = find_content_file(content_id, "textbook") is not None
    has_story = find_content_file(content_id, "story") is not None
    
    return {
        "textbook": has_textbook,
        "story": has_story
    }

@app.get("/api/learn/list", response_model=List[ContentItem])
async def list_content():
    """List all available content with status and progress"""
    
    # Get files in different stages
    inbox_files = get_inbox_files()
    processed_files = get_processed_files()
    available_content = get_available_content()
    
    # Load progress data
    progress_data = load_progress_data()
    
    items = []
    
    # Add available content (processed and has generated textbooks)
    for content_id in available_content:
        original_name = f"{content_id}.txt"
        
        # Get progress info (support both old and new format)
        progress = progress_data.get(content_id, {})
        
        # Handle new format with separate textbook/story progress
        if 'textbook' in progress or 'story' in progress:
            textbook_progress = progress.get('textbook', {}).get('viewedSections', 0)
            story_progress = progress.get('story', {}).get('viewedSections', 0)
            
            # Get total sections for each format to validate progress
            textbook_total = 10  # default
            story_total = 10     # default
            
            # Get actual total sections for each format
            for format_type in ["textbook", "story"]:
                content_file = find_content_file(content_id, format_type)
                if content_file and content_file.exists():
                    try:
                        with open(content_file, 'r', encoding='utf-8') as f:
                            content_data = json.load(f)
                            format_total = content_data.get('total_sections', 10)
                            if format_type == "textbook":
                                textbook_total = format_total
                            else:
                                story_total = format_total
                    except Exception as e:
                        logger.error(f"Error reading {format_type} file {content_file}: {e}")
            
            # Validate progress against format-specific totals and clamp if needed
            validated_textbook_progress = min(textbook_progress, textbook_total)
            validated_story_progress = min(story_progress, story_total)
            
            # Log if we had to clamp any progress
            if textbook_progress > textbook_total:
                logger.warning(f"Clamped textbook progress for {content_id}: {textbook_progress} -> {validated_textbook_progress} (max: {textbook_total})")
            if story_progress > story_total:
                logger.warning(f"Clamped story progress for {content_id}: {story_progress} -> {validated_story_progress} (max: {story_total})")
            
            # Use maximum validated progress for display
            viewed_sections = max(validated_textbook_progress, validated_story_progress)
            last_viewed = progress.get('textbook', {}).get('lastViewed') or progress.get('story', {}).get('lastViewed')
            
            # Use the total_sections from the format that has the maximum progress
            if validated_textbook_progress >= validated_story_progress:
                total_sections = textbook_total
            else:
                total_sections = story_total
        else:
            # Handle old format (backward compatibility)
            viewed_sections = progress.get('viewedSections', 0)
            last_viewed = progress.get('lastViewed')
            
            # Try to determine total sections from available format files
            total_sections = 10  # default
            
            # Try textbook first, then story
            for format_type in ["textbook", "story"]:
                content_file = find_content_file(content_id, format_type)
                if content_file and content_file.exists():
                    try:
                        with open(content_file, 'r', encoding='utf-8') as f:
                            content_data = json.load(f)
                            total_sections = content_data.get('total_sections', 10)
                            break  # Use the first valid file found
                    except Exception as e:
                        logger.error(f"Error reading {format_type} file {content_file}: {e}")
        
        items.append(ContentItem(
            id=content_id,
            name=content_id,
            displayName=format_display_name(content_id),
            status="available",
            totalSections=total_sections,
            viewedSections=viewed_sections,
            lastViewed=last_viewed
        ))
    
    # Add building content (in inbox but not yet available)
    for filename in inbox_files:
        content_id = filename.replace('.txt', '')
        if content_id not in available_content:
            items.append(ContentItem(
                id=content_id,
                name=content_id,
                displayName=format_display_name(content_id),
                status="building"
            ))
    
    logger.info(f"Listed {len(items)} content items: {len(available_content)} available, {len([i for i in items if i.status == 'building'])} building")
    return items

@app.get("/api/learn/{content_id}/formats")
async def get_content_formats(content_id: str):
    """Get available formats for a content item"""
    formats = get_available_formats(content_id)
    
    if not any(formats.values()):
        raise HTTPException(status_code=404, detail=f"No content found for '{content_id}'")
    
    return formats

@app.get("/api/learn/{content_id}/progress")
async def get_progress(content_id: str):
    """Get progress for a specific content item with format breakdown"""
    progress_data = load_progress_data()
    
    if content_id not in progress_data:
        return {
            "textbook": {"viewedSections": 0, "lastViewed": None},
            "story": {"viewedSections": 0, "lastViewed": None}
        }
    
    progress = progress_data[content_id]
    
    # Handle new format with separate textbook/story progress
    if 'textbook' in progress or 'story' in progress:
        textbook_data = progress.get('textbook', {"viewedSections": 0, "lastViewed": None})
        story_data = progress.get('story', {"viewedSections": 0, "lastViewed": None})
        
        # Ensure currentSection exists for each format
        if 'currentSection' not in textbook_data:
            textbook_data['currentSection'] = max(1, textbook_data.get('viewedSections', 0))
        if 'currentSection' not in story_data:
            story_data['currentSection'] = max(1, story_data.get('viewedSections', 0))
            
        return {
            "textbook": textbook_data,
            "story": story_data,
            "lastUsedFormat": progress.get('lastUsedFormat', 'textbook')
        }
    else:
        # Handle old format (backward compatibility) - assume old progress was textbook
        current_section = max(1, progress.get('viewedSections', 0))
        return {
            "textbook": {
                "viewedSections": progress.get('viewedSections', 0), 
                "lastViewed": progress.get('lastViewed'),
                "currentSection": current_section
            },
            "story": {
                "viewedSections": 0, 
                "lastViewed": None,
                "currentSection": 1
            },
            "lastUsedFormat": "textbook"
        }

@app.get("/api/learn/{content_id}", response_model=LearnContent)
async def get_content(content_id: str, format: str = "textbook"):
    """Get specific educational content in requested format"""
    
    # Validate format
    if format not in ["textbook", "story"]:
        raise HTTPException(status_code=400, detail="Format must be 'textbook' or 'story'")
    
    content_file = find_content_file(content_id, format)
    
    if not content_file or not content_file.exists():
        raise HTTPException(status_code=404, detail=f"Content '{content_id}' not found in '{format}' format")
    
    try:
        with open(content_file, 'r', encoding='utf-8') as f:
            content_data = json.load(f)
        
        logger.info(f"Served {format} content for {content_id}: {content_data.get('total_sections', 0)} sections")
        return LearnContent(**content_data)
        
    except Exception as e:
        logger.error(f"Error reading {format} file {content_file}: {e}")
        raise HTTPException(status_code=500, detail="Error reading content file")

@app.put("/api/learn/{content_id}/progress")
async def update_progress(content_id: str, progress_update: ProgressUpdate):
    """Update reading progress for specific content and format"""
    
    # Validate the update against actual content total_sections
    content_file = find_content_file(content_id, progress_update.format)
    if content_file and content_file.exists():
        try:
            with open(content_file, 'r', encoding='utf-8') as f:
                content_data = json.load(f)
                format_total_sections = content_data.get('total_sections', 10)
                
                # Validate that viewedSections doesn't exceed the format's total_sections
                if progress_update.viewedSections > format_total_sections:
                    logger.warning(f"Attempt to set {progress_update.format} progress to {progress_update.viewedSections} sections, but content only has {format_total_sections} sections")
                    return {
                        "success": False,
                        "error": f"Cannot set progress to {progress_update.viewedSections} sections. Content only has {format_total_sections} sections.",
                        "maxSections": format_total_sections
                    }
        except Exception as e:
            logger.error(f"Error validating progress update against {progress_update.format} file: {e}")
            # Continue without validation if file can't be read
    
    # Load current progress
    progress_data = load_progress_data()
    
    # Initialize content progress if not exists
    if content_id not in progress_data:
        progress_data[content_id] = {}
    
    # Initialize format structure if not exists
    if 'textbook' not in progress_data[content_id]:
        progress_data[content_id]['textbook'] = {'viewedSections': 0, 'lastViewed': None}
    if 'story' not in progress_data[content_id]:
        progress_data[content_id]['story'] = {'viewedSections': 0, 'lastViewed': None}
    
    # Check if this is the first time accessing this content (for automatic question generation)
    format_key = progress_update.format
    is_first_access = False
    
    if format_key in ['textbook', 'story']:
        previous_viewed_sections = progress_data[content_id][format_key].get('viewedSections', 0)
        is_first_access = previous_viewed_sections == 0 and progress_update.viewedSections > 0
        
        # Update progress for the specific format
        progress_data[content_id][format_key]['viewedSections'] = progress_update.viewedSections
        progress_data[content_id][format_key]['lastViewed'] = datetime.now().strftime('%Y-%m-%d')
        # Store the current section for this specific format
        progress_data[content_id][format_key]['currentSection'] = progress_update.currentSection
    
    # Update last used format only
    progress_data[content_id]['lastUsedFormat'] = format_key
    
    # Save updated progress
    save_progress_data(progress_data)
    
    # Trigger automatic question generation if this is first access
    if is_first_access:
        logger.info(f"First access detected for {content_id}, triggering automatic question generation")
        try:
            # Trigger initial easy questions asynchronously (don't wait for completion)
            asyncio.create_task(automatic_questions_service.trigger_initial_questions(content_id))
        except Exception as e:
            logger.error(f"Error triggering automatic question generation for {content_id}: {e}")
    
    # Calculate combined progress (max of both formats)
    textbook_progress = progress_data[content_id]['textbook']['viewedSections']
    story_progress = progress_data[content_id]['story']['viewedSections']
    combined_progress = max(textbook_progress, story_progress)
    
    logger.info(f"Updated {format_key} progress for {content_id}: {progress_update.viewedSections} sections viewed (combined: {combined_progress})")
    
    # Log xAPI statement for content navigation
    try:
        xapi_logger.log_content_navigation(
            content_name=content_id,
            content_type=format_key,  # "textbook" or "story"
            section=str(progress_update.currentSection)
        )
    except Exception as xapi_error:
        logger.error(f"Error logging xAPI statement for content navigation: {xapi_error}")
    
    return {
        "success": True, 
        "viewedSections": progress_update.viewedSections,
        "combinedProgress": combined_progress,
        "textbookProgress": textbook_progress,
        "storyProgress": story_progress,
        "lastUsedFormat": format_key,
        "currentSection": progress_update.currentSection
    }

@app.get("/api/practice/status")
async def get_practice_status():
    """Get smart practice status with generation progress and contextual messaging"""
    
    try:
        # Check if any content has been accessed in Learn
        accessed_content = automatic_questions_service.discover_accessed_content()
        has_accessed_content = len(accessed_content) > 0
        
        # Count available questions across all content
        practice_dir = Path("content/generated/practice")
        total_questions = 0
        
        if practice_dir.exists():
            for content_dir in practice_dir.iterdir():
                if content_dir.is_dir():
                    question_files = list(content_dir.glob("questions_*.json"))
                    for question_file in question_files:
                        try:
                            with open(question_file, 'r', encoding='utf-8') as f:
                                questions_data = json.load(f)
                            
                            # Count questions in all sections
                            for section in questions_data.get('questions', {}).values():
                                if isinstance(section, list):
                                    total_questions += len(section)
                                    
                        except Exception as e:
                            logger.error(f"Error reading question file {question_file}: {e}")
        
        # Determine generation status and message
        generation_status = "idle"
        message = ""
        
        if not has_accessed_content:
            # No content accessed yet
            generation_status = "idle"
            message = "Visit the Learn section first to explore some content. Practice questions are automatically generated when you read through topics for the first time!"
            
        elif has_accessed_content and total_questions < 8:  # Minimum expected questions per content
            # Content accessed but few questions available
            # Check if generation might be in progress
            progress_data = automatic_questions_service.load_progress_data()
            
            # Look for recent question generation activity
            recent_generation = False
            for content_id in accessed_content:
                if content_id in progress_data:
                    questions_data = progress_data[content_id].get("questions", {})
                    if questions_data.get("easy_generated", False):
                        recent_generation = True
                        break
            
            if recent_generation:
                generation_status = "in_progress"
                message = "New practice questions are being generated based on your learning progress. Check back soon!"
            else:
                generation_status = "failed"
                message = "Question generation is taking longer than expected. Try refreshing the page or visiting more content in the Learn section."
                
        else:
            # Questions are available
            generation_status = "completed"
            message = f"{total_questions} practice questions available across {len(accessed_content)} topic{'s' if len(accessed_content) != 1 else ''}!"
        
        return {
            "has_accessed_content": has_accessed_content,
            "content_accessed": accessed_content,
            "questions_available": total_questions,
            "generation_status": generation_status,
            "message": message
        }
        
    except Exception as e:
        logger.error(f"Error getting practice status: {e}")
        return {
            "has_accessed_content": False,
            "content_accessed": [],
            "questions_available": 0,
            "generation_status": "error",
            "message": "Unable to check practice questions status. Please try refreshing the page."
        }

@app.get("/api/practice/list")
async def list_practice_questions():
    """Get all available practice questions organized by content and type"""
    
    practice_dir = Path("content/generated/practice")
    if not practice_dir.exists():
        return []
    
    content_questions = []
    
    for content_dir in practice_dir.iterdir():
        if content_dir.is_dir():
            content_id = content_dir.name
            questions = []
            
            # Load questions from UUID-based system
            registry_file = content_dir / "questions_registry.json"
            questions_subdir = content_dir / "questions"
            
            if registry_file.exists() and questions_subdir.exists():
                try:
                    # Load registry
                    with open(registry_file, 'r', encoding='utf-8') as f:
                        registry_data = json.load(f)
                    
                    # Get default difficulty from metadata
                    default_difficulty = registry_data.get('metadata', {}).get('difficulty_level', 'medium')
                    
                    # Load each question from individual UUID files
                    for question_uuid, question_info in registry_data.get('questions', {}).items():
                        question_file = questions_subdir / f"{question_uuid}.json"
                        
                        if question_file.exists():
                            try:
                                with open(question_file, 'r', encoding='utf-8') as f:
                                    question_data = json.load(f)
                                
                                question_type = question_data.get('type', 'unknown')
                                question_text = question_data.get('text', '')
                                difficulty = question_data.get('difficulty', default_difficulty)
                                
                                # Handle different question types
                                if question_type == 'multiple_choice':
                                    # Convert options dict to array and handle correct answer
                                    options_dict = question_data.get('options', {})
                                    options_array = list(options_dict.values()) if isinstance(options_dict, dict) else []
                                    
                                    # Convert letter answer to actual text
                                    correct_answer = question_data.get('correct_answer', '')
                                    if isinstance(options_dict, dict) and correct_answer in options_dict:
                                        correct_answer = options_dict[correct_answer]
                                    
                                    questions.append({
                                        'id': question_uuid,
                                        'type': 'multiple_choice',
                                        'question': question_text,
                                        'options': options_array,
                                        'answer': correct_answer,
                                        'explanation': question_data.get('explanation', ''),
                                        'contentId': content_id,
                                        'difficulty': difficulty
                                    })
                                    
                                elif question_type == 'true_false':
                                    # Convert true/false to multiple choice format
                                    correct_answer_bool = question_data.get('correct_answer', True)
                                    if isinstance(correct_answer_bool, str):
                                        correct_answer_bool = correct_answer_bool.lower() == 'true'
                                    
                                    # Get student language from profile for localization
                                    from student_profile import get_current_student_profile
                                    profile = get_current_student_profile()
                                    language = profile.get('language', 'English')
                                    
                                    # Localize True/False options based on student's language
                                    if language == 'Spanish' or language == 'es':
                                        options_array = ['Verdadero', 'Falso']
                                        correct_answer = 'Verdadero' if correct_answer_bool else 'Falso'
                                    elif language == 'French' or language == 'fr':
                                        options_array = ['Vrai', 'Faux']
                                        correct_answer = 'Vrai' if correct_answer_bool else 'Faux'
                                    elif language == 'German' or language == 'de':
                                        options_array = ['Wahr', 'Falsch']
                                        correct_answer = 'Wahr' if correct_answer_bool else 'Falsch'
                                    elif language == 'Italian' or language == 'it':
                                        options_array = ['Vero', 'Falso']
                                        correct_answer = 'Vero' if correct_answer_bool else 'Falso'
                                    elif language == 'Portuguese' or language == 'pt':
                                        options_array = ['Verdadeiro', 'Falso']
                                        correct_answer = 'Verdadeiro' if correct_answer_bool else 'Falso'
                                    else:  # Default to English for 'English', 'en', or any other language
                                        options_array = ['True', 'False']
                                        correct_answer = 'True' if correct_answer_bool else 'False'
                                    
                                    questions.append({
                                        'id': question_uuid,
                                        'type': 'multiple_choice',  # Treat as multiple choice
                                        'question': question_text,
                                        'options': options_array,
                                        'answer': correct_answer,
                                        'explanation': question_data.get('explanation', ''),
                                        'contentId': content_id,
                                        'difficulty': difficulty
                                    })
                                    
                                elif question_type in ['fill_blank', 'short_answer', 'free_recall']:
                                    # Use correct_answer for fill_blank, sample_answer for others
                                    answer = question_data.get('correct_answer') if question_type == 'fill_blank' else question_data.get('sample_answer', '')
                                    
                                    questions.append({
                                        'id': question_uuid,
                                        'type': question_type,
                                        'question': question_text,
                                        'options': None,
                                        'answer': answer,
                                        'explanation': question_data.get('explanation', ''),
                                        'contentId': content_id,
                                        'difficulty': difficulty
                                    })
                                
                                logger.info(f"📝 Loaded UUID question: {question_uuid} ({question_type})")
                                    
                            except Exception as e:
                                logger.error(f"Error loading question file {question_file}: {e}")
                        
                except Exception as e:
                    logger.error(f"Error loading questions registry from {registry_file}: {e}")
            
            if questions:
                content_questions.append({
                    'contentId': content_id,
                    'displayName': format_display_name(content_id),
                    'questions': questions
                })
    
    return content_questions

def find_question_by_id(question_id: str, content_id: str):
    """Find a specific question by UUID within a content"""
    # Looking for question UUID in content
    
    practice_dir = Path("content/generated/practice")
    content_dir = practice_dir / content_id
    
    if not content_dir.exists():
        # Content directory does not exist
        return None
    
    # Check for UUID-based system
    registry_file = content_dir / "questions_registry.json"
    questions_subdir = content_dir / "questions"
    
    if registry_file.exists() and questions_subdir.exists():
        try:
            # Load registry
            with open(registry_file, 'r', encoding='utf-8') as f:
                registry_data = json.load(f)
            
            # Check if question UUID exists in registry
            if question_id in registry_data.get('questions', {}):
                question_file = questions_subdir / f"{question_id}.json"
                
                if question_file.exists():
                    # Load individual question file
                    with open(question_file, 'r', encoding='utf-8') as f:
                        question_data = json.load(f)
                    
                    # Found UUID question
                    # Question text logged
                    
                    # Return in same format as before for compatibility
                    return {
                        'question': {
                            'text': question_data.get('text', ''),
                            'correct_answer': question_data.get('correct_answer'),
                            'sample_answer': question_data.get('sample_answer'),
                            'options': question_data.get('options', {}),
                            'type': question_data.get('type')
                        },
                        'type': question_data.get('type'),
                        'metadata': {
                            'difficulty': question_data.get('difficulty'),
                            'source_content': question_data.get('source_content'),
                            'generated_at': question_data.get('generated_at')
                        }
                    }
                else:
                    # Question file not found
                    pass
            else:
                # Question UUID not found in registry
                pass
                
        except Exception as e:
            logger.error(f"Error searching UUID question system: {e}")
    
    # Question not found in content
    return None

@app.post("/api/practice/answer")
async def record_practice_answer(answer: PracticeAnswer):
    """Record a practice question answer and update progress with AI evaluation for open-ended questions"""
    
    practice_progress = load_practice_progress()
    today = datetime.now().strftime('%Y-%m-%d')
    
    evaluation_method = "automatic"
    feedback = None
    is_correct = answer.isCorrect  # Default from request
    
    # AI evaluation for open-ended questions
    if answer.questionType in ['fill_blank', 'short_answer', 'free_recall'] and answer.userAnswer:
        logger.info(f"Starting AI evaluation for {answer.questionType} question: {answer.questionId}")
        try:
            # Find the question data
            question_data = find_question_by_id(answer.questionId, answer.contentId)
            
            if question_data:
                question_text = question_data['question'].get('text', 'NO TEXT')
                logger.info(f"✅ VALIDATION: Found question for ID '{answer.questionId}'")
                logger.info(f"✅ VALIDATION: Question text: '{question_text[:100]}...'")
                logger.info(f"✅ VALIDATION: User answer: '{answer.userAnswer}'")
            else:
                logger.error(f"❌ VALIDATION: Question not found for ID '{answer.questionId}' in content '{answer.contentId}'")
                
            if question_data:
                # Get model service
                model_service = await get_model_service()
                
                # Get student profile from environment variables (always source of truth)
                env_profile = get_student_profile_from_env()
                
                # Prepare variables for evaluation prompt
                question_obj = question_data['question']
                expected_answer = question_obj.get('correct_answer') or question_obj.get('sample_answer', '')
                
                variables = {
                    'question_text': question_obj.get('text', ''),
                    'expected_answer': expected_answer,
                    'student_answer': answer.userAnswer,
                    'student_name': env_profile['student_name'],
                    'student_age': env_profile['student_age'],
                    'language': env_profile['language']
                }
                
                # Load the evaluation prompt template
                prompt_template = model_service.load_prompt('evaluate_answer')
                
                # Generate evaluation with AI
                # Use MAX_RETRIES from .env (default: 3) for better parsing reliability
                max_retries = int(os.getenv('MAX_RETRIES', 3))
                logger.info(f"AI evaluation will attempt up to {max_retries} retries for parsing")
                
                evaluation_result = model_service.generate(
                    prompt_template=prompt_template,
                    variables=variables,
                    parser_func=parse_answer_evaluation,
                    max_tokens=256,  # Short response
                    max_retries=max_retries
                )
                
                if evaluation_result and isinstance(evaluation_result, dict):
                    is_correct = evaluation_result.get('is_correct', False)
                    feedback = evaluation_result.get('feedback', 'Evaluación completada.')
                    evaluation_method = "ai"
                    logger.info(f"✓ AI evaluation successful for {answer.questionId}: {is_correct} - {feedback[:50]}...")
                elif evaluation_result and isinstance(evaluation_result, str):
                    # Parsing failed after all retries, got raw content
                    logger.warning(f"✗ AI evaluation parsing failed after {max_retries} attempts for {answer.questionId}")
                    logger.warning(f"Raw AI response: {evaluation_result[:200]}...")
                    
                    # For short_answer and free_recall, mark as incorrect with explanation
                    if answer.questionType in ['short_answer', 'free_recall']:
                        is_correct = False
                        feedback = "No se pudo evaluar automáticamente tu respuesta. Por favor, revisa que hayas respondido la pregunta completamente."
                        evaluation_method = "ai_failed"
                    else:
                        # For fill_blank, try simple fallback comparison
                        expected_answer = question_obj.get('correct_answer', '').lower().strip()
                        user_answer_clean = answer.userAnswer.lower().strip()
                        is_correct = expected_answer in user_answer_clean or user_answer_clean in expected_answer
                        feedback = "Evaluación automática básica aplicada."
                        evaluation_method = "fallback"
                        logger.info(f"Using fallback comparison for fill_blank: {is_correct}")
                else:
                    logger.error(f"✗ AI evaluation completely failed for {answer.questionId}, using manual assessment")
                    # Continue with original isCorrect from request
            else:
                logger.error(f"Question data not found for {answer.questionId} in content {answer.contentId}")
                    
        except Exception as e:
            logger.error(f"Exception during AI evaluation for {answer.questionId}: {str(e)}")
            logger.error(f"Question type: {answer.questionType}, User answer length: {len(answer.userAnswer) if answer.userAnswer else 0}")
            # Continue with manual evaluation as fallback
    
    # Initialize content progress if not exists
    if answer.contentId not in practice_progress:
        practice_progress[answer.contentId] = {}
    
    # Initialize question type progress if not exists
    if answer.questionType not in practice_progress[answer.contentId]:
        practice_progress[answer.contentId][answer.questionType] = {
            'correct': [],
            'total_attempted': [],
            'last_session': today
        }
    
    type_progress = practice_progress[answer.contentId][answer.questionType]
    
    # Update last session date
    type_progress['last_session'] = today
    
    # Add to attempted questions if not already there
    if answer.questionId not in type_progress['total_attempted']:
        type_progress['total_attempted'].append(answer.questionId)
    
    # Update correct answers based on AI evaluation or manual assessment
    if is_correct and answer.questionId not in type_progress['correct']:
        type_progress['correct'].append(answer.questionId)
    elif not is_correct and answer.questionId in type_progress['correct']:
        # Remove from correct if answered incorrectly (for daily reset scenarios)
        type_progress['correct'].remove(answer.questionId)
    
    # Save updated progress
    save_practice_progress(practice_progress)
    
    logger.info(f"✓ Recorded practice answer: {answer.contentId}/{answer.questionType}/{answer.questionId} = {is_correct} (method: {evaluation_method})")
    
    # Check for automatic question progression if answer was correct
    if is_correct:
        try:
            logger.info(f"Correct answer detected, checking for question progression for {answer.contentId}")
            # Check if difficulty level is completed and trigger next difficulty (async, don't wait)
            asyncio.create_task(automatic_questions_service.check_completion_and_progress(answer.contentId))
        except Exception as e:
            logger.error(f"Error checking automatic question progression for {answer.contentId}: {e}")
    
    # Log xAPI statement for question answered
    try:
        question_data = find_question_by_id(answer.questionId, answer.contentId)
        question_text = question_data['question'].get('text', 'Question not found') if question_data else 'Question not found'
        
        # Calculate score if applicable
        score_raw = 1 if is_correct else 0
        score_max = 1
        
        xapi_logger.log_question_answered(
            question_id=answer.questionId,
            question_text=question_text,
            student_answer=answer.userAnswer or "No answer provided",
            is_correct=is_correct,
            feedback=feedback or "No feedback provided",
            score_raw=score_raw,
            score_max=score_max
        )
    except Exception as e:
        logger.error(f"Error logging xAPI statement for question answer: {e}")
    
    return {
        "success": True,
        "isCorrect": is_correct,
        "evaluationMethod": evaluation_method,
        "feedback": feedback,
        "correct_count": len(type_progress['correct']),
        "attempted_count": len(type_progress['total_attempted'])
    }

@app.get("/api/practice/progress/{content_id}")
async def get_practice_progress(content_id: str):
    """Get practice progress for a specific content"""
    
    practice_progress = load_practice_progress()
    content_progress = practice_progress.get(content_id, {})
    
    # Get actual total questions available for this content
    practice_dir = Path("content/generated/practice")
    content_dir = practice_dir / content_id
    
    # Count total questions by type from the actual files
    total_by_type = {
        'multiple_choice': 0,
        'fill_blank': 0,
        'short_answer': 0,
        'free_recall': 0
    }
    
    if content_dir.exists():
        for question_file in content_dir.glob("questions_*.json"):
            try:
                with open(question_file, 'r', encoding='utf-8') as f:
                    question_data = json.load(f)
                    questions_obj = question_data.get('questions', {})
                    
                    # Count questions of each type
                    for q_type in ['multiple_choice', 'true_false', 'fill_blank', 'short_answer', 'free_recall']:
                        questions_of_type = questions_obj.get(q_type, [])
                        count = len(questions_of_type)
                        
                        # Group multiple_choice and true_false together
                        if q_type in ['multiple_choice', 'true_false']:
                            total_by_type['multiple_choice'] += count
                        elif q_type in total_by_type:
                            total_by_type[q_type] += count
                            
            except Exception as e:
                logger.error(f"Error counting questions from {question_file}: {e}")
    
    # Calculate stats for each question type
    type_stats = {}
    for question_type in ['multiple_choice', 'fill_blank', 'short_answer', 'free_recall']:
        if question_type in content_progress:
            type_progress = content_progress[question_type]
            type_stats[question_type] = {
                'correct': len(type_progress['correct']),
                'total': total_by_type[question_type],  # Use actual total from files
                'last_session': type_progress.get('last_session'),
                'correct_ids': type_progress['correct'],
                'attempted_ids': type_progress['total_attempted']
            }
        else:
            type_stats[question_type] = {
                'correct': 0,
                'total': total_by_type[question_type],  # Use actual total from files
                'last_session': None,
                'correct_ids': [],
                'attempted_ids': []
            }
    
    return type_stats


def find_challenge_by_id(challenge_id: str):
    """Find a specific challenge by its UUID"""
    # Looking for challenge_id
    
    experiment_dir = Path("content/generated/experiment")
    
    if not experiment_dir.exists():
        # Experiment directory does not exist
        return None
    
    # Only UUID system is supported
    if is_uuid_format(challenge_id):
        return find_challenge_by_uuid(challenge_id, experiment_dir)
    else:
        logger.error(f"🔍 ERROR: Invalid challenge ID format. Only UUIDs are supported: {challenge_id}")
        return None

def is_uuid_format(challenge_id: str) -> bool:
    """Check if challenge_id is in UUID format"""
    import re
    uuid_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'
    return bool(re.match(uuid_pattern, challenge_id, re.IGNORECASE))

def find_challenge_by_uuid(challenge_uuid: str, experiment_dir: Path):
    """Find challenge using new UUID-based system"""
    logger.info(f"🔍 UUID: Looking for challenge with UUID: {challenge_uuid}")
    
    # Load registry
    registry_file = experiment_dir / "challenges_registry.json"
    if not registry_file.exists():
        logger.warning(f"🔍 UUID: Registry not found: {registry_file}")
        return None
    
    try:
        with open(registry_file, 'r', encoding='utf-8') as f:
            registry = json.load(f)
        
        # Check if UUID exists in registry
        if challenge_uuid not in registry.get("challenges", {}):
            logger.warning(f"🔍 UUID: Challenge UUID not found in registry: {challenge_uuid}")
            return None
        
        challenge_info = registry["challenges"][challenge_uuid]
        challenge_file_path = experiment_dir / challenge_info["file"]
        
        logger.info(f"🔍 UUID: Loading challenge from: {challenge_file_path}")
        
        # Load individual challenge file
        with open(challenge_file_path, 'r', encoding='utf-8') as f:
            challenge_data = json.load(f)
        
        # With new UUID system, all challenge data is at root level
        challenge = {
            'title': challenge_data.get('title', ''),
            'description': challenge_data.get('description', ''),
            'learning_goals': challenge_data.get('learning_goals', ''),
            'deliverables': challenge_data.get('deliverables', ''),
            'type': challenge_data.get('type', 'experimental_challenge')
        }
        
        metadata = {
            'source_contents': challenge_data.get('source_contents', []),
            'interdisciplinary': challenge_data.get('interdisciplinary', False),
            'generated_at': challenge_data.get('generated_at', ''),
            'uuid': challenge_data.get('uuid', challenge_uuid)
        }
        
        logger.info(f"✅ UUID: Found challenge: {challenge.get('title', 'Unknown')[:100]}...")
        
        return {
            'challenge': challenge,
            'metadata': metadata,
            'uuid': challenge_uuid,
            'challenge_file': challenge_info["file"]
        }
        
    except Exception as e:
        logger.error(f"🔍 UUID: Error loading challenge {challenge_uuid}: {e}")
        return None



@app.get("/api/experiment/list")
async def list_experiment_challenges():
    """Get all available experiment challenges with unique IDs (supports both UUID and legacy systems)"""
    
    experiment_dir = Path("content/generated/experiment")
    if not experiment_dir.exists():
        return []
    
    challenges = []
    progress = load_experiment_progress()
    rejected_ids = set(progress.get('rejected', []))
    
    # Try UUID-based system first (new format)
    registry_file = experiment_dir / "challenges_registry.json"
    if registry_file.exists():
        logger.info("📋 Using UUID-based challenge system")
        try:
            with open(registry_file, 'r', encoding='utf-8') as f:
                registry = json.load(f)
            
            for challenge_uuid, challenge_info in registry.get("challenges", {}).items():
                # Skip if this challenge was rejected
                if challenge_uuid not in rejected_ids:
                    # Load individual challenge file for full details
                    challenge_file_path = experiment_dir / challenge_info["file"]
                    try:
                        with open(challenge_file_path, 'r', encoding='utf-8') as f:
                            challenge_data = json.load(f)
                        
                        # Read challenge data directly from root level (new UUID structure)
                        challenges.append({
                            'id': challenge_uuid,  # Use UUID as ID
                            'title': challenge_data.get('title', ''),
                            'description': challenge_data.get('description', ''),
                            'learning_goals': challenge_data.get('learning_goals', ''),
                            'deliverables': challenge_data.get('deliverables', ''),
                            'type': challenge_data.get('type', 'experimental_challenge'),
                            'uuid': challenge_uuid
                        })
                        
                    except Exception as e:
                        logger.error(f"Error loading challenge {challenge_uuid} from {challenge_file_path}: {e}")
                        continue
        
        except Exception as e:
            logger.error(f"Error loading challenges registry: {e}")
            return []
    
    else:
        # No registry found - UUID system is required
        logger.error("📋 ERROR: challenges_registry.json not found. UUID system is required.")
        return []
    
    logger.info(f"📋 Loaded {len(challenges)} challenges from UUID system")
    return challenges



@app.post("/api/experiment/decision")
async def record_experiment_decision(decision: ExperimentDecision):
    """Record a decision on an experiment challenge (accepted or rejected)"""
    
    progress = load_experiment_progress()
    today = datetime.now().strftime('%Y-%m-%d')
    
    # Update last session
    progress['last_session'] = today
    
    # Record the decision
    if decision.decision == "accepted":
        if decision.challengeId not in progress['accepted']:
            progress['accepted'].append(decision.challengeId)
        # Remove from rejected if it was there (shouldn't happen, but just in case)
        if decision.challengeId in progress['rejected']:
            progress['rejected'].remove(decision.challengeId)
    elif decision.decision == "rejected":
        if decision.challengeId not in progress['rejected']:
            progress['rejected'].append(decision.challengeId)
        # Remove from accepted if it was there (shouldn't happen, but just in case)
        if decision.challengeId in progress['accepted']:
            progress['accepted'].remove(decision.challengeId)
    
    # Save progress
    save_experiment_progress(progress)
    
    logger.info(f"Recorded experiment decision: {decision.challengeId} = {decision.decision}")
    
    # Check for challenge exhaustion after recording decision
    if decision.decision == "rejected":
        logger.info("Challenge rejected, checking for potential exhaustion")
        try:
            # Run exhaustion check in background (don't wait for completion)
            asyncio.create_task(automatic_questions_service.check_challenge_exhaustion_and_regenerate())
            logger.info("Challenge exhaustion check task created")
        except Exception as e:
            logger.error(f"Error creating challenge exhaustion check task: {e}")
    
    return {
        "success": True,
        "accepted_count": len(progress['accepted']),
        "rejected_count": len(progress['rejected'])
    }

@app.get("/api/experiment/progress")
async def get_experiment_progress():
    """Get experiment progress statistics"""
    
    progress = load_experiment_progress()
    
    return {
        "accepted": progress.get('accepted', []),
        "rejected": progress.get('rejected', []),
        "accepted_count": len(progress.get('accepted', [])),
        "rejected_count": len(progress.get('rejected', [])),
        "last_session": progress.get('last_session')
    }

@app.get("/api/experiment/status")
async def get_experiment_status():
    """Get smart experiment status with generation progress and contextual messaging"""
    
    try:
        # Check if any content has been accessed in Learn
        accessed_content = automatic_questions_service.discover_accessed_content()
        has_accessed_content = len(accessed_content) > 0
        
        # Get available challenges count
        experiment_dir = Path("content/generated/experiment")
        challenges_available = 0
        experiment_progress = load_experiment_progress()
        rejected_ids = set(experiment_progress.get('rejected', []))
        
        if experiment_dir.exists():
            registry_file = experiment_dir / "challenges_registry.json"
            if registry_file.exists():
                try:
                    with open(registry_file, 'r', encoding='utf-8') as f:
                        registry = json.load(f)
                    
                    # Count non-rejected challenges
                    for challenge_uuid in registry.get("challenges", {}):
                        if challenge_uuid not in rejected_ids:
                            challenges_available += 1
                            
                except Exception as e:
                    logger.error(f"Error reading challenges registry: {e}")
        
        # Determine generation status and message
        generation_status = "idle"
        message = ""
        
        if not has_accessed_content:
            # No content accessed yet
            generation_status = "idle"
            message = "Visit the Learn section first to explore some content. Experiment challenges are automatically generated when you read through topics for the first time!"
            
        else:
            # Check if we're in an exhaustion/regeneration scenario
            # If we have a very high rejection rate, we should show "generating" even if challenges exist
            rejected_count = len(experiment_progress.get('rejected', []))
            total_generated_challenges = 0
            
            # Count total challenges that have ever been generated (including rejected ones)
            if experiment_dir.exists():
                registry_file = experiment_dir / "challenges_registry.json"
                if registry_file.exists():
                    try:
                        with open(registry_file, 'r', encoding='utf-8') as f:
                            registry = json.load(f)
                        total_generated_challenges = len(registry.get("challenges", {}))
                    except Exception as e:
                        logger.error(f"Error reading challenges registry for total count: {e}")
            
            # Check if we're in exhaustion mode (high rejection rate)
            if total_generated_challenges > 0:
                rejection_rate = rejected_count / total_generated_challenges
                is_exhausted = rejection_rate > 0.8 and rejected_count >= 3
                
                # Also check for recent regeneration activity
                progress_data = automatic_questions_service.load_progress_data()
                recent_regeneration = False
                
                for key in progress_data:
                    if "challenges" in progress_data[key]:
                        challenges_data = progress_data[key]["challenges"]
                        generation_count = challenges_data.get("generation_count", 0)
                        if generation_count > 1:  # More than initial generation = regeneration
                            from datetime import datetime, timedelta
                            last_gen = challenges_data.get("last_generated")
                            if last_gen:
                                try:
                                    last_gen_time = datetime.fromisoformat(last_gen)
                                    if datetime.now() - last_gen_time < timedelta(minutes=15):
                                        recent_regeneration = True
                                        break
                                except:
                                    pass
                
                if is_exhausted and recent_regeneration:
                    generation_status = "in_progress"
                    message = "Generating fresh challenges based on your interests. This may take a few moments..."
                elif challenges_available == 0:
                    # No available challenges but content accessed - initial generation
                    progress_data = automatic_questions_service.load_progress_data()
                    recent_generation = False
                    for content_id in accessed_content:
                        if content_id in progress_data:
                            content_progress = progress_data[content_id]
                            if "questions" in content_progress and content_progress["questions"].get("easy_generated", False):
                                recent_generation = True
                                break
                    
                    if recent_generation:
                        generation_status = "in_progress"
                        message = "New challenges are being generated based on your learning progress. Check back soon!"
                    else:
                        generation_status = "failed"
                        message = "Challenge generation is taking longer than expected. Try refreshing the page or visiting more content in the Learn section."
                else:
                    # Challenges are available
                    generation_status = "completed"
                    message = f"{challenges_available} experimental challenge{'s' if challenges_available != 1 else ''} available!"
            else:
                # No challenges generated yet or no rejection data
                generation_status = "completed" if challenges_available > 0 else "idle"
                message = f"{challenges_available} experimental challenge{'s' if challenges_available != 1 else ''} available!" if challenges_available > 0 else "Visit the Learn section first to explore some content. Experiment challenges are automatically generated when you read through topics for the first time!"
        
        # Debug logging
        logger.info(f"🔍 Experiment Status Debug:")
        logger.info(f"  - Content accessed: {has_accessed_content} ({accessed_content})")
        logger.info(f"  - Challenges available (non-rejected): {challenges_available}")
        logger.info(f"  - Total rejected: {len(experiment_progress.get('rejected', []))}")
        logger.info(f"  - Generation status: {generation_status}")
        logger.info(f"  - Message: {message}")
        
        return {
            "has_accessed_content": has_accessed_content,
            "content_accessed": accessed_content,
            "challenges_available": challenges_available,
            "generation_status": generation_status,
            "message": message
        }
        
    except Exception as e:
        logger.error(f"Error getting experiment status: {e}")
        return {
            "has_accessed_content": False,
            "content_accessed": [],
            "challenges_available": 0,
            "generation_status": "error",
            "message": "Unable to check challenge status. Please try refreshing the page."
        }

@app.post("/api/experiment/save-session")
async def save_experiment_session(session: ExperimentSession):
    """Save work-in-progress experiment session data"""
    
    progress = load_experiment_progress()
    
    # Save current session data
    progress['current_session'] = {
        'challengeId': session.challengeId,
        'textContent': session.textContent,
        'activeTab': session.activeTab,
        'uploadedFiles': session.uploadedFiles,
        'currentCanvas': session.currentCanvas,
        'lastModified': datetime.now().isoformat()
    }
    
    save_experiment_progress(progress)
    
    logger.info(f"Saved experiment session for challenge: {session.challengeId}")
    
    return {"success": True, "lastModified": progress['current_session']['lastModified']}

@app.get("/api/experiment/session")
async def get_experiment_session():
    """Get current experiment session data"""
    
    progress = load_experiment_progress()
    current_session = progress.get('current_session')
    
    if not current_session:
        return {"hasSession": False}
    
    return {
        "hasSession": True,
        "challengeId": current_session.get('challengeId'),
        "textContent": current_session.get('textContent', ''),
        "activeTab": current_session.get('activeTab', 'text'),
        "uploadedFiles": current_session.get('uploadedFiles', []),
        "currentCanvas": current_session.get('currentCanvas'),
        "lastModified": current_session.get('lastModified')
    }

@app.delete("/api/experiment/session")
async def clear_experiment_session():
    """Clear current experiment session"""
    
    progress = load_experiment_progress()
    
    if 'current_session' in progress:
        del progress['current_session']
        save_experiment_progress(progress)
        logger.info("Cleared experiment session")
    
    return {"success": True}

@app.post("/api/experiment/upload")
async def upload_experiment_file(challengeId: str, file: UploadFile = File(...)):
    """Upload a file for an experiment challenge"""
    
    try:
        # Ensure uploads directory exists
        EXPERIMENT_UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
        
        # Create challenge-specific directory
        challenge_dir = EXPERIMENT_UPLOADS_DIR / challengeId
        challenge_dir.mkdir(exist_ok=True)
        
        # Save the uploaded file
        file_path = challenge_dir / file.filename
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        logger.info(f"Uploaded file {file.filename} for challenge {challengeId}")
        
        return {
            "success": True,
            "filename": file.filename,
            "fileSize": len(content),
            "filePath": str(file_path.relative_to(CONTENT_DIR))
        }
        
    except Exception as e:
        logger.error(f"Error uploading file: {e}")
        raise HTTPException(status_code=500, detail=f"Error uploading file: {str(e)}")

@app.get("/api/experiment/files/{challenge_id}")
async def list_experiment_files(challenge_id: str):
    """List uploaded files for a challenge"""
    
    challenge_dir = EXPERIMENT_UPLOADS_DIR / challenge_id
    
    if not challenge_dir.exists():
        return {"files": []}
    
    files = []
    for file_path in challenge_dir.iterdir():
        if file_path.is_file():
            files.append({
                "filename": file_path.name,
                "size": file_path.stat().st_size,
                "lastModified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
            })
    
    return {"files": files}

@app.delete("/api/experiment/files/{challenge_id}/{filename}")
async def delete_experiment_file(challenge_id: str, filename: str):
    """Delete an uploaded file for a challenge"""
    
    try:
        challenge_dir = EXPERIMENT_UPLOADS_DIR / challenge_id
        file_path = challenge_dir / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail=f"File '{filename}' not found")
        
        if not str(file_path.resolve()).startswith(str(challenge_dir.resolve())):
            raise HTTPException(status_code=400, detail="Invalid file path")
        
        file_path.unlink()  # Delete the file
        logger.info(f"Deleted file {filename} for challenge {challenge_id}")
        
        return {"success": True, "message": f"File '{filename}' deleted successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting file {filename}: {e}")
        raise HTTPException(status_code=500, detail=f"Error deleting file: {str(e)}")


@app.post("/api/experiment/submit")
async def submit_challenge(submission: ChallengeSubmission):
    """Evaluate challenge submission and provide feedback"""
    
    try:
        logger.info(f"Starting challenge evaluation for: {submission.challengeId}")
        
        # Find the challenge by ID
        challenge_data = find_challenge_by_id(submission.challengeId)
        if not challenge_data:
            raise HTTPException(status_code=404, detail=f"Challenge '{submission.challengeId}' not found")
        
        challenge = challenge_data['challenge']
        logger.info(f"✅ Found challenge: {challenge.get('title', 'Unknown')[:100]}...")
        
        # Get student profile from environment (always source of truth)
        env_profile = get_student_profile_from_env()
        
        # Collect all visual inputs for the model
        images_for_model = []
        
        # Add canvas drawing if present
        if submission.canvasData:
            images_for_model.append(submission.canvasData)
            logger.info("Added canvas drawing to visual inputs")
        
        # Add uploaded image files
        challenge_dir = EXPERIMENT_UPLOADS_DIR / submission.challengeId
        uploaded_images_count = 0
        
        if challenge_dir.exists():
            for file_path in challenge_dir.iterdir():
                if file_path.is_file() and file_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                    images_for_model.append(str(file_path))  # Add file path for processing
                    uploaded_images_count += 1
        
        logger.info(f"Prepared {len(images_for_model)} visual inputs for model ({uploaded_images_count} uploads + {'1 canvas' if submission.canvasData else '0 canvas'})")
        
        # Prepare variables for the evaluation prompt
        variables = {
            'challenge_title': challenge.get('title', ''),
            'challenge_description': challenge.get('description', ''),
            'challenge_deliverables': challenge.get('deliverables', ''),
            'text_content': submission.textContent or 'No text provided',
            'has_drawing': 'Yes' if submission.canvasData else 'No',
            'uploaded_images_count': str(uploaded_images_count),
            'student_name': env_profile['student_name'],
            'student_age': env_profile['student_age'],
            'student_course': env_profile['student_course'],
            'student_interests': env_profile['student_interests'],
            'language': env_profile['language']
        }
        
        # Get model service
        model_service = await get_model_service()
        
        # Load the evaluation prompt template
        prompt_template = model_service.load_prompt('evaluate_challenge')
        
        # Generate evaluation with AI (using visual capabilities if canvas/images present)
        logger.info("Generating challenge evaluation with AI...")
        from parsers import get_parser
        parser_func = get_parser('challenge_feedback')
        
        evaluation_result = model_service.generate(
            prompt_template=prompt_template,
            variables=variables,
            parser_func=parser_func,
            max_tokens=1024,
            max_retries=3,
            images=images_for_model if images_for_model else None
        )
        
        if not evaluation_result:
            raise HTTPException(status_code=500, detail="Failed to generate challenge feedback")
        
        logger.info(f"✅ Challenge evaluation successful: ready_to_submit={evaluation_result.get('ready_to_submit', False)}")
        
        # Save submission to persistent storage
        try:
            from submission_service import save_challenge_submission
            
            submission_id = save_challenge_submission(
                submission_data={
                    'challengeId': submission.challengeId,
                    'textContent': submission.textContent,
                    'canvasData': submission.canvasData
                },
                challenge_data=challenge,
                feedback_data=evaluation_result,
                source_file=challenge_data.get('source_file', 'unknown.json')
            )
            
            logger.info(f"✅ Submission saved with ID: {submission_id}")
            
            # Add submission ID to response
            evaluation_result['submission_id'] = submission_id
            
        except Exception as save_error:
            logger.error(f"Error saving submission: {save_error}")
            # Continue with response even if submission save fails
        
        # Return feedback to frontend
        return {
            "success": True,
            "feedback": evaluation_result,
            "challengeId": submission.challengeId
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error evaluating challenge: {e}")
        raise HTTPException(status_code=500, detail=f"Error evaluating challenge: {str(e)}")


@app.post("/api/experiment/submit-async")
async def submit_challenge_async(submission: ChallengeSubmission):
    """Submit challenge for asynchronous feedback processing - returns immediately with task ID"""
    
    try:
        logger.info(f"Starting async challenge evaluation for: {submission.challengeId}")
        
        # Find the challenge by ID
        challenge_data = find_challenge_by_id(submission.challengeId)
        if not challenge_data:
            raise HTTPException(status_code=404, detail=f"Challenge '{submission.challengeId}' not found")
        
        challenge = challenge_data['challenge']
        logger.info(f"✅ Found challenge: {challenge.get('title', 'Unknown')[:100]}...")
        
        # Get student profile from environment (always source of truth)
        env_profile = get_student_profile_from_env()
        
        # Collect all visual inputs for the model
        images_for_model = []
        
        # Add canvas drawing if present
        if submission.canvasData:
            images_for_model.append(submission.canvasData)
            logger.info("Added canvas drawing to visual inputs")
        
        # Add uploaded image files
        challenge_dir = EXPERIMENT_UPLOADS_DIR / submission.challengeId
        uploaded_images_count = 0
        
        if challenge_dir.exists():
            for file_path in challenge_dir.iterdir():
                if file_path.is_file() and file_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                    images_for_model.append(str(file_path))  # Add file path for processing
                    uploaded_images_count += 1
        
        logger.info(f"Prepared {len(images_for_model)} visual inputs for model ({uploaded_images_count} uploads + {'1 canvas' if submission.canvasData else '0 canvas'})")
        
        # Prepare variables for the evaluation prompt
        variables = {
            'challenge_title': challenge.get('title', ''),
            'challenge_description': challenge.get('description', ''),
            'challenge_deliverables': challenge.get('deliverables', ''),
            'text_content': submission.textContent or 'No text provided',
            'has_drawing': 'Yes' if submission.canvasData else 'No',
            'uploaded_images_count': str(uploaded_images_count),
            'student_name': env_profile['student_name'],
            'student_age': env_profile['student_age'],
            'student_course': env_profile['student_course'],
            'student_interests': env_profile['student_interests'],
            'language': env_profile['language']
        }
        
        # Create async feedback task
        task_id = create_feedback_task(
            challenge_data=challenge_data,
            submission_data={
                'challengeId': submission.challengeId,
                'textContent': submission.textContent,
                'canvasData': submission.canvasData
            },
            variables=variables,
            images=images_for_model if images_for_model else None,
            source_file=challenge_data.get('source_file', 'unknown.json')
        )
        
        logger.info(f"✅ Async feedback task created: {task_id}")
        
        # Return task ID immediately
        return {
            "success": True,
            "task_id": task_id,
            "status": TASK_STATUS_PENDING,
            "message": "Feedback task created. Check status with the task ID.",
            "challengeId": submission.challengeId
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating async feedback task: {e}")
        raise HTTPException(status_code=500, detail=f"Error creating feedback task: {str(e)}")


@app.get("/api/experiment/feedback/{task_id}")
async def get_feedback_status(task_id: str):
    """Get the status and result of an async feedback task"""
    
    try:
        logger.info(f"Checking feedback status for task: {task_id}")
        
        task_status = get_task_status(task_id)
        
        if not task_status:
            raise HTTPException(status_code=404, detail=f"Task '{task_id}' not found")
        
        # Clean up old tasks periodically (every 100 requests)
        import random
        if random.randint(1, 100) == 1:
            cleanup_old_tasks(hours=24)
        
        response = {
            "success": True,
            "task_id": task_id,
            "status": task_status['status'],
            "created_at": task_status.get('created_at'),
            "challenge_id": task_status.get('challenge_id'),
            "challenge_title": task_status.get('challenge_title')
        }
        
        # Add status-specific information
        if task_status['status'] == TASK_STATUS_PENDING:
            response.update({
                "queue_position": task_status.get('queue_position', 1),
                "estimated_wait_minutes": round(task_status.get('estimated_wait_minutes', 2), 1),
                "message": f"In queue (position {task_status.get('queue_position', 1)}). Estimated wait: {round(task_status.get('estimated_wait_minutes', 2), 1)} minutes."
            })
        elif task_status['status'] == TASK_STATUS_PROCESSING:
            response.update({
                "started_at": task_status.get('started_at'),
                "message": "Generating feedback with AI model..."
            })
        elif task_status['status'] == TASK_STATUS_COMPLETED:
            response.update({
                "feedback": task_status.get('result'),
                "completed_at": task_status.get('completed_at'),
                "processing_time_seconds": task_status.get('processing_time_seconds'),
                "message": "Feedback ready!"
            })
        elif task_status['status'] == TASK_STATUS_ERROR:
            response.update({
                "error": task_status.get('error'),
                "completed_at": task_status.get('completed_at'),
                "message": "Error generating feedback. Please try again."
            })
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting feedback status: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting feedback status: {str(e)}")


@app.post("/api/experiment/feedback-decision")
async def record_feedback_decision(decision: ChallengeFeedbackDecision):
    """Record student's decision on challenge feedback (Continue improving vs Complete challenge)"""
    
    try:
        logger.info(f"Recording feedback decision for challenge {decision.challengeId}: continueRefining={decision.continueRefining}")
        
        # Get task status to retrieve feedback data
        task_status = get_task_status(decision.taskId)
        if not task_status or task_status['status'] != TASK_STATUS_COMPLETED:
            raise HTTPException(status_code=404, detail=f"Task '{decision.taskId}' not found or not completed")
        
        result = task_status.get('result', {})
        
        # Find the challenge data
        challenge_data = find_challenge_by_id(decision.challengeId)
        if not challenge_data:
            raise HTTPException(status_code=404, detail=f"Challenge '{decision.challengeId}' not found")
        
        challenge = challenge_data['challenge']
        
        # Log xAPI statement for challenge submission with decision
        try:
            submission_id = result.get('submission_id', 'unknown')
            is_final_submission = not decision.continueRefining  # Final if NOT continuing to refine
            
            # Get submission content from result or task
            submission_content = result.get('text_content', 'No text content available')
            
            # Determine submission type
            submission_type = "text"
            has_drawing = result.get('has_drawing', False)
            uploaded_images_count = result.get('uploaded_images_count', 0)
            
            if has_drawing:
                submission_type += "_with_drawing"
            if uploaded_images_count > 0:
                submission_type += "_with_images"
            
            xapi_logger.log_challenge_submitted(
                challenge_id=decision.challengeId,
                challenge_title=challenge.get('title', 'Unknown Challenge'),
                submission_id=submission_id,
                submission_content=submission_content,
                ai_feedback=result.get('feedback', 'No feedback available'),
                is_final_submission=is_final_submission,
                submission_type=submission_type
            )
            
            logger.info(f"✓ xAPI statement logged for challenge decision: final_submission={is_final_submission}")
            
        except Exception as xapi_error:
            logger.error(f"Error logging xAPI statement for challenge decision: {xapi_error}")
        
        return {
            "success": True,
            "challenge_id": decision.challengeId,
            "continue_refining": decision.continueRefining,
            "final_submission": is_final_submission,
            "message": "Decision recorded successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error recording feedback decision: {e}")
        raise HTTPException(status_code=500, detail=f"Error recording feedback decision: {str(e)}")


@app.get("/api/experiment/queue-stats")
async def get_feedback_queue_stats():
    """Get statistics about the feedback processing queue"""
    
    try:
        stats = get_queue_stats()
        
        return {
            "success": True,
            "stats": stats,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting queue stats: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting queue stats: {str(e)}")


@app.get("/api/content/status")
async def get_content_status():
    """Get real-time status of content processing"""
    
    inbox_files = get_inbox_files()
    processed_files = get_processed_files()
    available_content = get_available_content()
    
    # Get processing queue status
    queue_size = processing_queue.qsize() if processing_queue else 0
    
    return {
        "inbox_count": len(inbox_files),
        "inbox_files": inbox_files,
        "processed_count": len(processed_files),
        "processed_files": processed_files,
        "available_count": len(available_content),
        "available_content": available_content,
        "processing_queue_size": queue_size,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/discover/analyze")
async def start_discovery_investigation(request: DiscoveryAnalyzeRequest):
    """
    Start a new discovery investigation with button-based questions
    Supports both text questions and audio transcription
    """
    try:
        logger.info("Starting discovery investigation...")
        
        # Get discovery service
        discovery_service = await get_discovery_service()
        
        # Get student profile from environment variables (always source of truth)
        env_profile = get_student_profile_from_env()
        
        # Handle audio transcription if provided
        question_text = request.questionText
        if request.audioData and not question_text:
            try:
                # Decode base64 audio data
                audio_bytes = base64.b64decode(request.audioData)
                question_text = await discovery_service.transcribe_audio(audio_bytes)
                logger.info(f"Audio transcribed: '{question_text[:100]}...'")
            except Exception as e:
                logger.error(f"Audio transcription failed: {e}")
                question_text = "Could not transcribe audio. Please type your question."
        
        if not question_text:
            raise HTTPException(status_code=400, detail="Either questionText or audioData must be provided")
        
        # Start initial investigation
        investigation_result = await discovery_service.start_discovery_investigation(
            image_data=request.imageData,
            question_text=question_text,
            student_profile=env_profile
        )
        
        if not investigation_result:
            raise HTTPException(status_code=500, detail="Failed to start discovery investigation")
        
        logger.info(f"✓ Discovery investigation started: {investigation_result['investigation_id']}")
        
        return {
            "success": True,
            "investigation_id": investigation_result['investigation_id'],
            "subject_identified": investigation_result['subject_identified'],
            "learning_intent": investigation_result['learning_intent'],
            "contextual_intro": investigation_result['contextual_intro'],
            "guiding_questions": investigation_result['guiding_questions'],
            "question_count": investigation_result['question_count'],
            "question_limit": investigation_result['question_limit'],
            "transcribed_question": question_text if request.audioData else None
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting discovery investigation: {e}")
        raise HTTPException(status_code=500, detail=f"Error starting investigation: {str(e)}")


@app.post("/api/discover/question")
async def process_question_selection(request: DiscoveryQuestionRequest):
    """
    Process student's question selection and generate follow-up questions or trigger reveal
    """
    try:
        logger.info(f"Processing question selection for investigation: {request.sessionId}")
        
        # Get discovery service
        discovery_service = await get_discovery_service()
        
        # Get student profile from environment variables
        env_profile = get_student_profile_from_env()
        
        # Process question selection
        selection_result = await discovery_service.process_question_selection(
            investigation_id=request.sessionId,  # Using sessionId field for investigation_id
            selected_question=request.responseText,  # Using responseText field for selected question
            student_profile=env_profile
        )
        
        if not selection_result:
            raise HTTPException(status_code=500, detail="Failed to process question selection")
        
        logger.info(f"✓ Question selection processed for investigation: {request.sessionId}")
        
        return {
            "success": True,
            **selection_result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing question selection: {e}")
        raise HTTPException(status_code=500, detail=f"Error processing selection: {str(e)}")


@app.post("/api/discover/reveal")
async def reveal_answer_options(request: DiscoveryRevealRequest):
    """
    Generate final answer options for accordion display after question limit reached
    """
    try:
        logger.info(f"Revealing answer options for investigation: {request.investigationId}")
        
        # Get discovery service
        discovery_service = await get_discovery_service()
        
        # Get student profile from environment variables
        env_profile = get_student_profile_from_env()
        
        # Generate reveal options
        reveal_result = await discovery_service.reveal_answer_options(
            investigation_id=request.investigationId,
            student_profile=env_profile
        )
        
        if not reveal_result:
            raise HTTPException(status_code=500, detail="Failed to generate answer options")
        
        logger.info(f"✓ Answer options revealed for investigation: {request.investigationId}")
        
        return {
            "success": True,
            **reveal_result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error revealing answer options: {e}")
        raise HTTPException(status_code=500, detail=f"Error revealing options: {str(e)}")


@app.post("/api/discover/complete")
async def complete_investigation(request: DiscoveryCompleteRequest):
    """
    Complete the investigation with student's final answer choice
    """
    try:
        logger.info(f"Completing investigation: {request.investigationId} with answer: {request.selectedAnswer}")
        
        # Get discovery service
        discovery_service = await get_discovery_service()
        
        # Complete investigation
        completion_result = await discovery_service.complete_investigation(
            investigation_id=request.investigationId,
            selected_answer=request.selectedAnswer
        )
        
        if not completion_result:
            raise HTTPException(status_code=500, detail="Failed to complete investigation")
        
        logger.info(f"✓ Investigation completed: {request.investigationId}")
        
        return {
            "success": True,
            **completion_result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error completing investigation: {e}")
        raise HTTPException(status_code=500, detail=f"Error completing investigation: {str(e)}")


@app.get("/api/discover/data")
async def get_discovery_data_for_tutor(limit: int = 10):
    """
    Get completed discovery investigations for tutor synchronization
    """
    try:
        logger.info(f"Retrieving discovery data for tutor (limit: {limit})")
        
        # Get discovery service
        discovery_service = await get_discovery_service()
        
        # Get investigation data
        investigations = discovery_service.get_investigation_data_for_tutor(limit=limit)
        
        logger.info(f"✓ Retrieved {len(investigations)} completed investigations for tutor")
        
        return {
            "success": True,
            "investigations": investigations,
            "total_count": len(investigations)
        }
        
    except Exception as e:
        logger.error(f"Error getting discovery data: {e}")
        raise HTTPException(status_code=500, detail=f"Error retrieving data: {str(e)}")


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/api/events/stream")
async def event_stream():
    """Server-Sent Events endpoint for real-time content updates"""
    
    async def generate_events():
        client_queue = asyncio.Queue(maxsize=10)
        sse_manager.add_client(client_queue)
        
        try:
            # Send initial connection event
            yield f"data: {json.dumps({'type': 'connected', 'timestamp': datetime.now().isoformat()})}\n\n"
            
            while True:
                try:
                    # Wait for events with timeout using asyncio
                    event_data = await asyncio.wait_for(client_queue.get(), timeout=30.0)
                    yield f"data: {json.dumps(event_data)}\n\n"
                except asyncio.TimeoutError:
                    # Send heartbeat to keep connection alive
                    yield f"data: {json.dumps({'type': 'heartbeat', 'timestamp': datetime.now().isoformat()})}\n\n"
                except Exception as e:
                    logger.error(f"SSE error: {e}")
                    break
                    
        except Exception as e:
            logger.error(f"SSE connection error: {e}")
        finally:
            sse_manager.remove_client(client_queue)
    
    return StreamingResponse(
        generate_events(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control"
        }
    )

# Sync endpoints

@app.get("/api/sync/check-tutor")
async def check_tutor_service():
    """Check if tutor service is available for synchronization"""
    global previous_sync_status
    
    try:
        # Check if sync_client is available
        if 'sync_client' not in globals():
            logger.error("API: sync_client not imported or available")
            current_status = False
        else:
            logger.info(f"API: Checking tutor service availability at {sync_client.tutor_url}")
            current_status = sync_client.discover_tutor_service()
        
        # Check for status change and emit SSE event if changed
        if previous_sync_status is not None and previous_sync_status != current_status:
            logger.info(f"API: Sync status changed from {previous_sync_status} to {current_status}")
            try:
                await sse_manager.notify_sync_change(current_status)
            except Exception as sse_error:
                logger.error(f"API: Failed to send SSE sync notification: {sse_error}")
        
        # Update previous status
        previous_sync_status = current_status
        
        if 'sync_client' not in globals():
            return {
                "success": False,
                "available": False,
                "error": "Sync client not available - import error",
                "timestamp": datetime.now().isoformat()
            }
        
        # Get current student info dynamically
        student_id, student_name = sync_client._get_current_student_info()
        
        response_data = {
            "success": True,
            "available": current_status,
            "tutor_url": sync_client.tutor_url,
            "student_id": student_id,
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"API: Tutor service check result: {response_data}")
        return response_data
        
    except NameError as e:
        logger.error(f"API: sync_client not defined: {e}")
        current_status = False
        
        # Check for status change and emit SSE event if changed
        if previous_sync_status is not None and previous_sync_status != current_status:
            logger.info(f"API: Sync status changed from {previous_sync_status} to {current_status} (error)")
            try:
                await sse_manager.notify_sync_change(current_status)
            except Exception as sse_error:
                logger.error(f"API: Failed to send SSE sync notification: {sse_error}")
        
        # Update previous status
        previous_sync_status = current_status
        
        return {
            "success": False,
            "available": False,
            "error": f"Sync client not defined: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"API: Error checking tutor service: {e}")
        current_status = False
        
        # Check for status change and emit SSE event if changed
        if previous_sync_status is not None and previous_sync_status != current_status:
            logger.info(f"API: Sync status changed from {previous_sync_status} to {current_status} (exception)")
            try:
                await sse_manager.notify_sync_change(current_status)
            except Exception as sse_error:
                logger.error(f"API: Failed to send SSE sync notification: {sse_error}")
        
        # Update previous status
        previous_sync_status = current_status
        
        error_response = {
            "success": False,
            "available": False,
            "error": str(e),
            "tutor_url": getattr(sync_client, 'tutor_url', 'unknown') if 'sync_client' in globals() else 'unknown',
            "timestamp": datetime.now().isoformat()
        }
        return error_response

@app.post("/api/sync/notify-status-change")
async def notify_sync_status_change(request: dict):
    """Receive sync status change notification from tutor-app"""
    try:
        sync_enabled = request.get("sync_enabled", False)
        logger.info(f"API: Received sync status notification from tutor: {sync_enabled}")
        
        # Update global sync status immediately
        global previous_sync_status
        previous_sync_status = sync_enabled
        
        # Emit SSE event immediately to all connected clients
        try:
            await sse_manager.notify_sync_change(sync_enabled)
            logger.info(f"API: Successfully sent SSE sync notification: {sync_enabled}")
        except Exception as sse_error:
            logger.error(f"API: Failed to send SSE sync notification: {sse_error}")
        
        return {
            "success": True,
            "sync_enabled": sync_enabled,
            "message": f"Sync status updated to {'enabled' if sync_enabled else 'disabled'}",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"API: Error processing sync status notification: {e}")
        return {
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.post("/api/sync/to-tutor")
async def sync_to_tutor(request: dict = None):
    """Synchronize student data to tutor service (with optional delta sync)"""
    try:
        # Check if tutor service is available
        if not sync_client.discover_tutor_service():
            raise HTTPException(status_code=503, detail="Tutor service not available")
        
        # Extract last_sync timestamp from request if provided
        last_sync = None
        if request and 'last_sync' in request:
            last_sync = request['last_sync']
        
        # Perform synchronization
        sync_result = await sync_client.sync_to_tutor(last_sync=last_sync)
        
        if sync_result['success']:
            logger.info(f"Sync to tutor completed successfully")
            return {
                "success": True,
                "message": sync_result['message'],
                "assigned_content": sync_result.get('assigned_content', []),
                "removed_content": sync_result.get('removed_content', []),
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail=sync_result['message'])
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error syncing to tutor: {e}")
        raise HTTPException(status_code=500, detail=f"Sync failed: {str(e)}")

# Student Profile Management Endpoints

@app.get("/api/profile/current", response_model=ProfileResponse)
async def get_student_profile():
    """Get current student profile from environment variables"""
    try:
        profile = get_current_profile()
        return ProfileResponse(
            profile=profile,
            success=True
        )
    except Exception as e:
        logger.error(f"Error getting student profile: {e}")
        return ProfileResponse(
            profile=None,
            success=False,
            error=str(e)
        )

@app.post("/api/profile/update", response_model=ProfileResponse)
async def update_student_profile(request: ProfileUpdateRequest):
    """Update student profile and sync with .env file"""
    try:
        # Validate student ID format
        if not request.profile.id or len(request.profile.id) != 6 or not request.profile.id.isdigit():
            raise HTTPException(status_code=400, detail="Student ID must be exactly 6 digits")
        
        # Validate required fields
        if not request.profile.name.strip():
            raise HTTPException(status_code=400, detail="Student name is required")
        
        if request.profile.age < 10 or request.profile.age > 18:
            raise HTTPException(status_code=400, detail="Student age must be between 10 and 18")
        
        # Save profile to JSON file
        save_profile_json(request.profile)
        
        # Try to register with tutor-app (non-blocking)
        try:
            await register_student_with_tutor(request.profile)
        except Exception as e:
            logger.warning(f"Could not register with tutor-app: {e}")
        
        return ProfileResponse(
            profile=request.profile,
            success=True
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating student profile: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to update profile: {str(e)}")

@app.post("/api/profile/validate-id", response_model=StudentIdValidationResponse)
async def validate_student_id(request: StudentIdValidationRequest):
    """Validate if student ID is available and properly formatted"""
    try:
        # Basic format validation
        if not request.id or len(request.id) != 6 or not request.id.isdigit():
            return StudentIdValidationResponse(
                available=False,
                error="Student ID must be exactly 6 digits"
            )
        
        # Check availability with tutor-app
        is_available = await validate_student_id_with_tutor(request.id)
        
        if not is_available:
            return StudentIdValidationResponse(
                available=False,
                error="This student ID is already in use"
            )
        
        return StudentIdValidationResponse(available=True)
        
    except Exception as e:
        logger.error(f"Error validating student ID: {e}")
        return StudentIdValidationResponse(
            available=True,  # Default to available if validation fails
            error="Could not validate ID with tutor service"
        )

@app.post("/api/profile/register-with-tutor")
async def register_with_tutor():
    """Register current student profile with tutor-app"""
    try:
        profile = get_current_profile()
        if not profile:
            raise HTTPException(status_code=400, detail="No student profile found")
        
        success = await register_student_with_tutor(profile)
        
        if success:
            return {
                "success": True,
                "message": f"Student {profile.id} registered successfully with tutor-app"
            }
        else:
            return {
                "success": False,
                "message": "Failed to register with tutor-app"
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error registering with tutor: {e}")
        raise HTTPException(status_code=500, detail=f"Registration failed: {str(e)}")


if __name__ == "__main__":
    # Ensure required directories exist
    for directory in [INBOX_DIR, PROCESSED_DIR, TEXTBOOKS_DIR, STORIES_DIR, EXPERIMENT_UPLOADS_DIR]:
        directory.mkdir(parents=True, exist_ok=True)
    
    logger.info("Starting Student App API server...")
    logger.info(f"Serving content from: {CONTENT_DIR.resolve()}")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")